{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle as pkl\n",
    "\n",
    "import string\n",
    "import logging\n",
    "\n",
    "from nlstruct.utils import torch_clone\n",
    "from nlstruct.utils import torch_global as tg\n",
    "from nlstruct.dataloaders import load_from_brat\n",
    "from nlstruct.collections import Batcher\n",
    "\n",
    "sys.path.insert(0,'./deep_multilingual_normalization')\n",
    "\n",
    "from deep_multilingual_normalization.preprocess import preprocess, load_quaero\n",
    "from deep_multilingual_normalization.train import train_step1, train_step2, clear\n",
    "from deep_multilingual_normalization.eval import predict, compute_scores\n",
    "\n",
    "from notebook_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_PATH = '/home/ytaille/christel_g'\n",
    "PREDICT_DATA_PATH = '/home/ytaille/data/resources/mantra/Mantra-GSC/English/Medline_EN_FR_ec22-cui-best_man'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CUDA devices: 1\n",
      "Current device: cuda:0\n",
      "Using cache /home/ytaille/christel_g/cache/preprocess_training_data/84f2788785216393\n",
      "Loading /home/ytaille/christel_g/cache/preprocess_training_data/84f2788785216393/output.pkl... \n",
      "Loading MRCONSO...\n",
      "Deduplicating MRCONSO...\n",
      "French synonyms: 389221\n",
      "French labels: 152156\n",
      "Quaero mentions: 5714\n",
      "Mirrored labels: 152828\n",
      "Queried english labels: 152828\n",
      "Total deduplicated synonyms: 1430324\n",
      "Total deduplicated labels: 152828\n",
      "Will train vocabulary for label\n",
      "Will train vocabulary for group\n",
      "Will train vocabulary for source\n",
      "Will train vocabulary for token\n",
      "Discovered existing vocabulary (105879 entities) for token\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized split, with given vocabulary and no unk\n",
      "2828\n",
      "Total deduplicated synonyms: 1430324\n",
      "Total deduplicated labels: 152828\n",
      "Will train vocabulary for label\n",
      "Will train vocabulary for group\n",
      "Will train vocabulary for source\n",
      "Will train vocabulary for token\n",
      "Discovered existing vocabulary (105879 entities) for token\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Quaero mentions: 16283\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized label, with given vocabulary and no unk\n",
      "Normalized quaero_source, with given vocabulary and no unk\n",
      "Normalized token, with given vocabulary and no unk\n",
      "Normalized group, with given vocabulary and no unk\n",
      "Normalized source, with given vocabulary and no unk\n",
      "Using cache /home/ytaille/data/cache/norm/paper/train_step1/0493c15ed2180d03\n",
      "epoch | train_loss | train_acc | val_loss | val_acc | val_map | acc_emea | acc_medline |        lr |   norm |     step |    dur(s)\n",
      "    1 |     \u001b[32m5.7155\u001b[0m |    \u001b[32m0.3243\u001b[0m |   \u001b[32m1.6889\u001b[0m |  \u001b[32m0.7140\u001b[0m |  \u001b[32m0.7866\u001b[0m |   \u001b[32m0.7855\u001b[0m |      \u001b[32m0.6497\u001b[0m | 8.000e-03 |   8.85 |    10100 | 1529.3542\n",
      "    2 |     \u001b[32m1.8668\u001b[0m |    \u001b[32m0.7272\u001b[0m |   \u001b[32m1.0415\u001b[0m |  \u001b[32m0.8085\u001b[0m |  \u001b[32m0.8688\u001b[0m |   \u001b[32m0.8708\u001b[0m |      \u001b[32m0.7525\u001b[0m | 7.704e-03 |  11.85 |    20200 | 1535.0108\n",
      "    3 |     \u001b[32m1.1027\u001b[0m |    \u001b[32m0.8528\u001b[0m |   \u001b[32m0.7614\u001b[0m |  \u001b[32m0.8511\u001b[0m |  \u001b[32m0.9027\u001b[0m |   \u001b[32m0.8852\u001b[0m |      \u001b[32m0.8204\u001b[0m | 7.111e-03 |  13.71 |    30300 | 1535.1948\n",
      "    4 |     \u001b[32m0.7899\u001b[0m |    \u001b[32m0.9012\u001b[0m |   \u001b[32m0.6436\u001b[0m |  \u001b[32m0.8607\u001b[0m |  \u001b[32m0.9129\u001b[0m |   \u001b[32m0.8870\u001b[0m |      \u001b[32m0.8370\u001b[0m | 6.519e-03 |  14.98 |    40400 | 1533.3442\n",
      "    5 |     \u001b[32m0.6193\u001b[0m |    \u001b[32m0.9259\u001b[0m |   \u001b[32m0.5656\u001b[0m |  \u001b[32m0.8749\u001b[0m |  \u001b[32m0.9239\u001b[0m |   \u001b[32m0.9036\u001b[0m |      \u001b[32m0.8490\u001b[0m | 5.926e-03 |  15.89 |    50500 | 1532.6588\n",
      "    6 |     \u001b[32m0.5115\u001b[0m |    \u001b[32m0.9402\u001b[0m |   \u001b[32m0.5100\u001b[0m |  \u001b[32m0.8827\u001b[0m |  \u001b[32m0.9299\u001b[0m |   \u001b[32m0.9129\u001b[0m |      \u001b[32m0.8556\u001b[0m | 5.333e-03 |  16.56 |    60600 | 1533.1525\n",
      "    7 |     \u001b[32m0.4369\u001b[0m |    \u001b[32m0.9496\u001b[0m |   \u001b[32m0.4774\u001b[0m |  \u001b[31m0.8801\u001b[0m |  \u001b[32m0.9303\u001b[0m |   \u001b[31m0.9018\u001b[0m |      \u001b[32m0.8606\u001b[0m | 4.741e-03 |  17.05 |    70700 | 1535.1828\n",
      "    8 |     \u001b[32m0.3830\u001b[0m |    \u001b[32m0.9556\u001b[0m |   \u001b[32m0.4349\u001b[0m |  \u001b[32m0.8910\u001b[0m |  \u001b[32m0.9373\u001b[0m |   \u001b[32m0.9173\u001b[0m |      \u001b[32m0.8673\u001b[0m | 4.148e-03 |  17.41 |    80800 | 1534.4039\n",
      "    9 |     \u001b[32m0.3420\u001b[0m |    \u001b[32m0.9602\u001b[0m |   \u001b[32m0.4103\u001b[0m |  \u001b[31m0.8906\u001b[0m |  \u001b[32m0.9379\u001b[0m |   \u001b[31m0.9162\u001b[0m |      \u001b[32m0.8676\u001b[0m | 3.556e-03 |  17.68 |    90900 | 1535.8676\n",
      "   10 |     \u001b[32m0.3103\u001b[0m |    \u001b[32m0.9636\u001b[0m |   \u001b[32m0.3908\u001b[0m |  \u001b[32m0.8957\u001b[0m |  \u001b[32m0.9413\u001b[0m |   \u001b[32m0.9213\u001b[0m |      \u001b[32m0.8726\u001b[0m | 2.963e-03 |  17.86 |   101000 | 1533.0423\n",
      "   11 |     \u001b[32m0.2845\u001b[0m |    \u001b[32m0.9664\u001b[0m |   \u001b[32m0.3758\u001b[0m |  \u001b[32m0.8985\u001b[0m |  \u001b[32m0.9431\u001b[0m |   \u001b[32m0.9254\u001b[0m |      \u001b[32m0.8743\u001b[0m | 2.370e-03 |  17.99 |   111100 | 1535.2655\n",
      "   12 |     \u001b[32m0.2643\u001b[0m |    \u001b[32m0.9686\u001b[0m |   \u001b[32m0.3629\u001b[0m |  \u001b[32m0.8988\u001b[0m |  \u001b[32m0.9436\u001b[0m |   \u001b[31m0.9217\u001b[0m |      \u001b[32m0.8782\u001b[0m | 1.778e-03 |  18.08 |   121200 | 1533.8976\n",
      "   13 |     \u001b[32m0.2472\u001b[0m |    \u001b[32m0.9705\u001b[0m |   \u001b[32m0.3509\u001b[0m |  \u001b[32m0.9027\u001b[0m |  \u001b[32m0.9459\u001b[0m |   \u001b[31m0.9221\u001b[0m |      \u001b[32m0.8852\u001b[0m | 1.185e-03 |  18.13 |   131300 | 1532.5618\n",
      "   14 |     \u001b[32m0.2339\u001b[0m |    \u001b[32m0.9720\u001b[0m |   \u001b[32m0.3435\u001b[0m |  \u001b[32m0.9046\u001b[0m |  \u001b[32m0.9470\u001b[0m |   \u001b[31m0.9250\u001b[0m |      \u001b[32m0.8862\u001b[0m | 5.926e-04 |  18.15 |   141400 | 1531.4463\n",
      "   15 |     \u001b[32m0.2231\u001b[0m |    \u001b[32m0.9734\u001b[0m |   \u001b[31m0.3435\u001b[0m |  \u001b[31m0.9025\u001b[0m |  \u001b[31m0.9460\u001b[0m |   \u001b[31m0.9239\u001b[0m |      \u001b[31m0.8832\u001b[0m | 0.000e+00 |  18.16 |   151500 | 1531.1572\n",
      "Available CUDA devices: 1\n",
      "Current device: cuda:0\n",
      "Using cache /home/ytaille/data/cache/preprocess_training_data/84f2788785216393\n",
      "Loading /home/ytaille/data/cache/preprocess_training_data/84f2788785216393/output.pkl... \n",
      "Loading MRCONSO...\n",
      "Deduplicating MRCONSO...\n",
      "French synonyms: 389221\n",
      "French labels: 152156\n",
      "Quaero mentions: 5714\n",
      "Mirrored labels: 152828\n",
      "Queried english labels: 152828\n",
      "Total deduplicated synonyms: 1430324\n",
      "Total deduplicated labels: 152828\n",
      "Will train vocabulary for label\n",
      "Will train vocabulary for group\n",
      "Will train vocabulary for source\n",
      "Will train vocabulary for token\n",
      "Discovered existing vocabulary (105879 entities) for token\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized split, with given vocabulary and no unk\n",
      "2828\n",
      "Total deduplicated synonyms: 1430324\n",
      "Total deduplicated labels: 152828\n",
      "Will train vocabulary for label\n",
      "Will train vocabulary for group\n",
      "Will train vocabulary for source\n",
      "Will train vocabulary for token\n",
      "Discovered existing vocabulary (105879 entities) for token\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Quaero mentions: 16283\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized label, with given vocabulary and no unk\n",
      "Normalized quaero_source, with given vocabulary and no unk\n",
      "Normalized token, with given vocabulary and no unk\n",
      "Normalized group, with given vocabulary and no unk\n",
      "Normalized source, with given vocabulary and no unk\n",
      "Using cache /home/ytaille/data/cache/norm/paper/train_step1/0493c15ed2180d03\n",
      "epoch | train_loss | train_acc | val_loss | val_acc | val_map | acc_emea | acc_medline |        lr |   norm |     step |    dur(s)\n",
      "    1 |     \u001b[32m5.7155\u001b[0m |    \u001b[32m0.3243\u001b[0m |   \u001b[32m1.6889\u001b[0m |  \u001b[32m0.7140\u001b[0m |  \u001b[32m0.7866\u001b[0m |   \u001b[32m0.7855\u001b[0m |      \u001b[32m0.6497\u001b[0m | 8.000e-03 |   8.85 |    10100 | 1529.3542\n",
      "    2 |     \u001b[32m1.8668\u001b[0m |    \u001b[32m0.7272\u001b[0m |   \u001b[32m1.0415\u001b[0m |  \u001b[32m0.8085\u001b[0m |  \u001b[32m0.8688\u001b[0m |   \u001b[32m0.8708\u001b[0m |      \u001b[32m0.7525\u001b[0m | 7.704e-03 |  11.85 |    20200 | 1535.0108\n",
      "    3 |     \u001b[32m1.1027\u001b[0m |    \u001b[32m0.8528\u001b[0m |   \u001b[32m0.7614\u001b[0m |  \u001b[32m0.8511\u001b[0m |  \u001b[32m0.9027\u001b[0m |   \u001b[32m0.8852\u001b[0m |      \u001b[32m0.8204\u001b[0m | 7.111e-03 |  13.71 |    30300 | 1535.1948\n",
      "    4 |     \u001b[32m0.7899\u001b[0m |    \u001b[32m0.9012\u001b[0m |   \u001b[32m0.6436\u001b[0m |  \u001b[32m0.8607\u001b[0m |  \u001b[32m0.9129\u001b[0m |   \u001b[32m0.8870\u001b[0m |      \u001b[32m0.8370\u001b[0m | 6.519e-03 |  14.98 |    40400 | 1533.3442\n",
      "    5 |     \u001b[32m0.6193\u001b[0m |    \u001b[32m0.9259\u001b[0m |   \u001b[32m0.5656\u001b[0m |  \u001b[32m0.8749\u001b[0m |  \u001b[32m0.9239\u001b[0m |   \u001b[32m0.9036\u001b[0m |      \u001b[32m0.8490\u001b[0m | 5.926e-03 |  15.89 |    50500 | 1532.6588\n",
      "    6 |     \u001b[32m0.5115\u001b[0m |    \u001b[32m0.9402\u001b[0m |   \u001b[32m0.5100\u001b[0m |  \u001b[32m0.8827\u001b[0m |  \u001b[32m0.9299\u001b[0m |   \u001b[32m0.9129\u001b[0m |      \u001b[32m0.8556\u001b[0m | 5.333e-03 |  16.56 |    60600 | 1533.1525\n",
      "    7 |     \u001b[32m0.4369\u001b[0m |    \u001b[32m0.9496\u001b[0m |   \u001b[32m0.4774\u001b[0m |  \u001b[31m0.8801\u001b[0m |  \u001b[32m0.9303\u001b[0m |   \u001b[31m0.9018\u001b[0m |      \u001b[32m0.8606\u001b[0m | 4.741e-03 |  17.05 |    70700 | 1535.1828\n",
      "    8 |     \u001b[32m0.3830\u001b[0m |    \u001b[32m0.9556\u001b[0m |   \u001b[32m0.4349\u001b[0m |  \u001b[32m0.8910\u001b[0m |  \u001b[32m0.9373\u001b[0m |   \u001b[32m0.9173\u001b[0m |      \u001b[32m0.8673\u001b[0m | 4.148e-03 |  17.41 |    80800 | 1534.4039\n",
      "    9 |     \u001b[32m0.3420\u001b[0m |    \u001b[32m0.9602\u001b[0m |   \u001b[32m0.4103\u001b[0m |  \u001b[31m0.8906\u001b[0m |  \u001b[32m0.9379\u001b[0m |   \u001b[31m0.9162\u001b[0m |      \u001b[32m0.8676\u001b[0m | 3.556e-03 |  17.68 |    90900 | 1535.8676\n",
      "   10 |     \u001b[32m0.3103\u001b[0m |    \u001b[32m0.9636\u001b[0m |   \u001b[32m0.3908\u001b[0m |  \u001b[32m0.8957\u001b[0m |  \u001b[32m0.9413\u001b[0m |   \u001b[32m0.9213\u001b[0m |      \u001b[32m0.8726\u001b[0m | 2.963e-03 |  17.86 |   101000 | 1533.0423\n",
      "   11 |     \u001b[32m0.2845\u001b[0m |    \u001b[32m0.9664\u001b[0m |   \u001b[32m0.3758\u001b[0m |  \u001b[32m0.8985\u001b[0m |  \u001b[32m0.9431\u001b[0m |   \u001b[32m0.9254\u001b[0m |      \u001b[32m0.8743\u001b[0m | 2.370e-03 |  17.99 |   111100 | 1535.2655\n",
      "   12 |     \u001b[32m0.2643\u001b[0m |    \u001b[32m0.9686\u001b[0m |   \u001b[32m0.3629\u001b[0m |  \u001b[32m0.8988\u001b[0m |  \u001b[32m0.9436\u001b[0m |   \u001b[31m0.9217\u001b[0m |      \u001b[32m0.8782\u001b[0m | 1.778e-03 |  18.08 |   121200 | 1533.8976\n",
      "   13 |     \u001b[32m0.2472\u001b[0m |    \u001b[32m0.9705\u001b[0m |   \u001b[32m0.3509\u001b[0m |  \u001b[32m0.9027\u001b[0m |  \u001b[32m0.9459\u001b[0m |   \u001b[31m0.9221\u001b[0m |      \u001b[32m0.8852\u001b[0m | 1.185e-03 |  18.13 |   131300 | 1532.5618\n",
      "   14 |     \u001b[32m0.2339\u001b[0m |    \u001b[32m0.9720\u001b[0m |   \u001b[32m0.3435\u001b[0m |  \u001b[32m0.9046\u001b[0m |  \u001b[32m0.9470\u001b[0m |   \u001b[31m0.9250\u001b[0m |      \u001b[32m0.8862\u001b[0m | 5.926e-04 |  18.15 |   141400 | 1531.4463\n",
      "   15 |     \u001b[32m0.2231\u001b[0m |    \u001b[32m0.9734\u001b[0m |   \u001b[31m0.3435\u001b[0m |  \u001b[31m0.9025\u001b[0m |  \u001b[31m0.9460\u001b[0m |   \u001b[31m0.9239\u001b[0m |      \u001b[31m0.8832\u001b[0m | 0.000e+00 |  18.16 |   151500 | 1531.1572\n",
      "Available CUDA devices: 1\n",
      "Current device: cuda:0\n",
      "Using cache /home/ytaille/data/cache/preprocess_training_data/84f2788785216393\n",
      "Loading /home/ytaille/data/cache/preprocess_training_data/84f2788785216393/output.pkl... \n",
      "lle/data/cache/preprocess_training_data/84f2788785216393\n",
      "Loading /home/ytaille/data/cache/preprocess_training_data/84f2788785216393/output.pkl... \n",
      "Quaero mentions: 16283\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized label, with given vocabulary and no unk\n",
      "Normalized quaero_source, with given vocabulary and no unk\n",
      "Normalized token, with given vocabulary and no unk\n",
      "Normalized group, with given vocabulary and no unk\n",
      "Normalized source, with given vocabulary and no unk\n",
      "Using cache /home/ytaille/data/cache/norm/paper/train_step1/0493c15ed2180d03\n",
      "Loading /home/ytaille/data/cache/norm/paper/train_step1/0493c15ed2180d03/history.yaml... \n",
      "epoch | train_loss | train_acc | val_loss | val_acc | val_map | acc_emea | acc_medline |        lr |   norm |     step |    dur(s)\n",
      "    1 |     \u001b[32m5.7155\u001b[0m |    \u001b[32m0.3243\u001b[0m |   \u001b[32m1.6889\u001b[0m |  \u001b[32m0.7140\u001b[0m |  \u001b[32m0.7866\u001b[0m |   \u001b[32m0.7855\u001b[0m |      \u001b[32m0.6497\u001b[0m | 8.000e-03 |   8.85 |    10100 | 1529.3542\n",
      "    2 |     \u001b[32m1.8668\u001b[0m |    \u001b[32m0.7272\u001b[0m |   \u001b[32m1.0415\u001b[0m |  \u001b[32m0.8085\u001b[0m |  \u001b[32m0.8688\u001b[0m |   \u001b[32m0.8708\u001b[0m |      \u001b[32m0.7525\u001b[0m | 7.704e-03 |  11.85 |    20200 | 1535.0108\n",
      "    3 |     \u001b[32m1.1027\u001b[0m |    \u001b[32m0.8528\u001b[0m |   \u001b[32m0.7614\u001b[0m |  \u001b[32m0.8511\u001b[0m |  \u001b[32m0.9027\u001b[0m |   \u001b[32m0.8852\u001b[0m |      \u001b[32m0.8204\u001b[0m | 7.111e-03 |  13.71 |    30300 | 1535.1948\n",
      "    4 |     \u001b[32m0.7899\u001b[0m |    \u001b[32m0.9012\u001b[0m |   \u001b[32m0.6436\u001b[0m |  \u001b[32m0.8607\u001b[0m |  \u001b[32m0.9129\u001b[0m |   \u001b[32m0.8870\u001b[0m |      \u001b[32m0.8370\u001b[0m | 6.519e-03 |  14.98 |    40400 | 1533.3442\n",
      "    5 |     \u001b[32m0.6193\u001b[0m |    \u001b[32m0.9259\u001b[0m |   \u001b[32m0.5656\u001b[0m |  \u001b[32m0.8749\u001b[0m |  \u001b[32m0.9239\u001b[0m |   \u001b[32m0.9036\u001b[0m |      \u001b[32m0.8490\u001b[0m | 5.926e-03 |  15.89 |    50500 | 1532.6588\n",
      "    6 |     \u001b[32m0.5115\u001b[0m |    \u001b[32m0.9402\u001b[0m |   \u001b[32m0.5100\u001b[0m |  \u001b[32m0.8827\u001b[0m |  \u001b[32m0.9299\u001b[0m |   \u001b[32m0.9129\u001b[0m |      \u001b[32m0.8556\u001b[0m | 5.333e-03 |  16.56 |    60600 | 1533.1525\n",
      "    7 |     \u001b[32m0.4369\u001b[0m |    \u001b[32m0.9496\u001b[0m |   \u001b[32m0.4774\u001b[0m |  \u001b[31m0.8801\u001b[0m |  \u001b[32m0.9303\u001b[0m |   \u001b[31m0.9018\u001b[0m |      \u001b[32m0.8606\u001b[0m | 4.741e-03 |  17.05 |    70700 | 1535.1828\n",
      "    8 |     \u001b[32m0.3830\u001b[0m |    \u001b[32m0.9556\u001b[0m |   \u001b[32m0.4349\u001b[0m |  \u001b[32m0.8910\u001b[0m |  \u001b[32m0.9373\u001b[0m |   \u001b[32m0.9173\u001b[0m |      \u001b[32m0.8673\u001b[0m | 4.148e-03 |  17.41 |    80800 | 1534.4039\n",
      "    9 |     \u001b[32m0.3420\u001b[0m |    \u001b[32m0.9602\u001b[0m |   \u001b[32m0.4103\u001b[0m |  \u001b[31m0.8906\u001b[0m |  \u001b[32m0.9379\u001b[0m |   \u001b[31m0.9162\u001b[0m |      \u001b[32m0.8676\u001b[0m | 3.556e-03 |  17.68 |    90900 | 1535.8676\n",
      "   10 |     \u001b[32m0.3103\u001b[0m |    \u001b[32m0.9636\u001b[0m |   \u001b[32m0.3908\u001b[0m |  \u001b[32m0.8957\u001b[0m |  \u001b[32m0.9413\u001b[0m |   \u001b[32m0.9213\u001b[0m |      \u001b[32m0.8726\u001b[0m | 2.963e-03 |  17.86 |   101000 | 1533.0423\n",
      "   11 |     \u001b[32m0.2845\u001b[0m |    \u001b[32m0.9664\u001b[0m |   \u001b[32m0.3758\u001b[0m |  \u001b[32m0.8985\u001b[0m |  \u001b[32m0.9431\u001b[0m |   \u001b[32m0.9254\u001b[0m |      \u001b[32m0.8743\u001b[0m | 2.370e-03 |  17.99 |   111100 | 1535.2655\n",
      "   12 |     \u001b[32m0.2643\u001b[0m |    \u001b[32m0.9686\u001b[0m |   \u001b[32m0.3629\u001b[0m |  \u001b[32m0.8988\u001b[0m |  \u001b[32m0.9436\u001b[0m |   \u001b[31m0.9217\u001b[0m |      \u001b[32m0.8782\u001b[0m | 1.778e-03 |  18.08 |   121200 | 1533.8976\n",
      "   13 |     \u001b[32m0.2472\u001b[0m |    \u001b[32m0.9705\u001b[0m |   \u001b[32m0.3509\u001b[0m |  \u001b[32m0.9027\u001b[0m |  \u001b[32m0.9459\u001b[0m |   \u001b[31m0.9221\u001b[0m |      \u001b[32m0.8852\u001b[0m | 1.185e-03 |  18.13 |   131300 | 1532.5618\n",
      "   14 |     \u001b[32m0.2339\u001b[0m |    \u001b[32m0.9720\u001b[0m |   \u001b[32m0.3435\u001b[0m |  \u001b[32m0.9046\u001b[0m |  \u001b[32m0.9470\u001b[0m |   \u001b[31m0.9250\u001b[0m |      \u001b[32m0.8862\u001b[0m | 5.926e-04 |  18.15 |   141400 | 1531.4463\n",
      "   15 |     \u001b[32m0.2231\u001b[0m |    \u001b[32m0.9734\u001b[0m |   \u001b[31m0.3435\u001b[0m |  \u001b[31m0.9025\u001b[0m |  \u001b[31m0.9460\u001b[0m |   \u001b[31m0.9239\u001b[0m |      \u001b[31m0.8832\u001b[0m | 0.000e+00 |  18.16 |   151500 | 1531.1572\n",
      "Loading /home/ytaille/data/cache/norm/paper/train_step1/0493c15ed2180d03/checkpoint-15.pt... \n",
      "Model restored to its best self.state: 15\n",
      "Using cache /home/ytaille/data/cache/preprocess_training_data/0bc03df4bddfd317\n",
      "Loading /home/ytaille/data/cache/preprocess_training_data/0bc03df4bddfd317/output.pkl... \n",
      "Loading MRCONSO...\n",
      "Deduplicating MRCONSO...\n",
      "French synonyms: 174317\n",
      "French labels: 84172\n",
      "Quaero mentions: 5714\n",
      "Mirrored labels: 84845\n",
      "Queried english labels: 84845\n",
      "Adding all english concepts from SABs: ['CHV', 'SNOMEDCT_US', 'MTH', 'NCI', 'MSH']\n",
      "Total deduplicated synonyms: 2461183\n",
      "Total deduplicated labels: 766764\n",
      "Will train vocabulary for label\n",
      "Will train vocabulary for group\n",
      "Will train vocabulary for source\n",
      "Will train vocabulary for token\n",
      "Discovered existing vocabulary (105879 entities) for token\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Using cache /home/ytaille/data/cache/preprocess_training_data/4d2b03f45f152b1f\n",
      "Loading MRCONSO...\n",
      "Deduplicating MRCONSO...\n",
      "French synonyms: 389221\n",
      "French labels: 152156\n",
      "Quaero mentions: 5714\n",
      "Mirrored labels: 152828\n",
      "Queried english labels: 152828\n",
      "Adding all english concepts from SABs: ['CHV', 'SNOMEDCT_US', 'MTH', 'NCI', 'MSH']\n",
      "Total deduplicated synonyms: 3641321\n",
      "Total deduplicated labels: 1054783\n",
      "Will train vocabulary for label\n",
      "Will train vocabulary for group\n",
      "Will train vocabulary for source\n",
      "Will train vocabulary for token\n",
      "Discovered existing vocabulary (105879 entities) for token\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Quaero mentions: 16283\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized label, with given vocabulary and no unk\n",
      "Normalized quaero_source, with given vocabulary and no unk\n",
      "Normalized token, with given vocabulary and no unk\n",
      "Normalized group, with given vocabulary and no unk\n",
      "Normalized source, with given vocabulary and no unk\n",
      "Using cache /home/ytaille/data/cache/train_step2/21f90ac54a5fed7e\n",
      "epoch | val_acc | val_loss | acc_emea | acc_medline | train_loss | rescale |        lr\n",
      "    0 |  \u001b[32m0.6899\u001b[0m |     None |   \u001b[32m0.6725\u001b[0m |      \u001b[32m0.7033\u001b[0m |       None |    None |      None\n",
      "train_loss | rescale |        lr\n",
      "    0 |  \u001b[32m0.6899\u001b[0m |     None |   \u001b[32m0.6725\u001b[0m |      \u001b[32m0.7033\u001b[0m |       None |    None |      None\n",
      "Quaero mentions: 16283\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized label, with given vocabulary and no unk\n",
      "Normalized quaero_source, with given vocabulary and no unk\n",
      "Normalized token, with given vocabulary and no unk\n",
      "Normalized group, with given vocabulary and no unk\n",
      "Normalized source, with given vocabulary and no unk\n",
      "Using cache /home/ytaille/christel_g/cache/train_step1/0493c15ed2180d03\n",
      "Loading /home/ytaille/christel_g/cache/train_step1/0493c15ed2180d03/history.yaml... \n",
      "epoch | train_loss | train_acc | val_loss | val_acc | val_map | acc_emea | acc_medline |        lr |   norm |     step |    dur(s)\n",
      "    1 |     \u001b[32m5.7155\u001b[0m |    \u001b[32m0.3243\u001b[0m |   \u001b[32m1.6889\u001b[0m |  \u001b[32m0.7140\u001b[0m |  \u001b[32m0.7866\u001b[0m |   \u001b[32m0.7855\u001b[0m |      \u001b[32m0.6497\u001b[0m | 8.000e-03 |   8.85 |    10100 | 1529.3542\n",
      "    2 |     \u001b[32m1.8668\u001b[0m |    \u001b[32m0.7272\u001b[0m |   \u001b[32m1.0415\u001b[0m |  \u001b[32m0.8085\u001b[0m |  \u001b[32m0.8688\u001b[0m |   \u001b[32m0.8708\u001b[0m |      \u001b[32m0.7525\u001b[0m | 7.704e-03 |  11.85 |    20200 | 1535.0108\n",
      "    3 |     \u001b[32m1.1027\u001b[0m |    \u001b[32m0.8528\u001b[0m |   \u001b[32m0.7614\u001b[0m |  \u001b[32m0.8511\u001b[0m |  \u001b[32m0.9027\u001b[0m |   \u001b[32m0.8852\u001b[0m |      \u001b[32m0.8204\u001b[0m | 7.111e-03 |  13.71 |    30300 | 1535.1948\n",
      "    4 |     \u001b[32m0.7899\u001b[0m |    \u001b[32m0.9012\u001b[0m |   \u001b[32m0.6436\u001b[0m |  \u001b[32m0.8607\u001b[0m |  \u001b[32m0.9129\u001b[0m |   \u001b[32m0.8870\u001b[0m |      \u001b[32m0.8370\u001b[0m | 6.519e-03 |  14.98 |    40400 | 1533.3442\n",
      "    5 |     \u001b[32m0.6193\u001b[0m |    \u001b[32m0.9259\u001b[0m |   \u001b[32m0.5656\u001b[0m |  \u001b[32m0.8749\u001b[0m |  \u001b[32m0.9239\u001b[0m |   \u001b[32m0.9036\u001b[0m |      \u001b[32m0.8490\u001b[0m | 5.926e-03 |  15.89 |    50500 | 1532.6588\n",
      "    6 |     \u001b[32m0.5115\u001b[0m |    \u001b[32m0.9402\u001b[0m |   \u001b[32m0.5100\u001b[0m |  \u001b[32m0.8827\u001b[0m |  \u001b[32m0.9299\u001b[0m |   \u001b[32m0.9129\u001b[0m |      \u001b[32m0.8556\u001b[0m | 5.333e-03 |  16.56 |    60600 | 1533.1525\n",
      "    7 |     \u001b[32m0.4369\u001b[0m |    \u001b[32m0.9496\u001b[0m |   \u001b[32m0.4774\u001b[0m |  \u001b[31m0.8801\u001b[0m |  \u001b[32m0.9303\u001b[0m |   \u001b[31m0.9018\u001b[0m |      \u001b[32m0.8606\u001b[0m | 4.741e-03 |  17.05 |    70700 | 1535.1828\n",
      "    8 |     \u001b[32m0.3830\u001b[0m |    \u001b[32m0.9556\u001b[0m |   \u001b[32m0.4349\u001b[0m |  \u001b[32m0.8910\u001b[0m |  \u001b[32m0.9373\u001b[0m |   \u001b[32m0.9173\u001b[0m |      \u001b[32m0.8673\u001b[0m | 4.148e-03 |  17.41 |    80800 | 1534.4039\n",
      "    9 |     \u001b[32m0.3420\u001b[0m |    \u001b[32m0.9602\u001b[0m |   \u001b[32m0.4103\u001b[0m |  \u001b[31m0.8906\u001b[0m |  \u001b[32m0.9379\u001b[0m |   \u001b[31m0.9162\u001b[0m |      \u001b[32m0.8676\u001b[0m | 3.556e-03 |  17.68 |    90900 | 1535.8676\n",
      "   10 |     \u001b[32m0.3103\u001b[0m |    \u001b[32m0.9636\u001b[0m |   \u001b[32m0.3908\u001b[0m |  \u001b[32m0.8957\u001b[0m |  \u001b[32m0.9413\u001b[0m |   \u001b[32m0.9213\u001b[0m |      \u001b[32m0.8726\u001b[0m | 2.963e-03 |  17.86 |   101000 | 1533.0423\n",
      "   11 |     \u001b[32m0.2845\u001b[0m |    \u001b[32m0.9664\u001b[0m |   \u001b[32m0.3758\u001b[0m |  \u001b[32m0.8985\u001b[0m |  \u001b[32m0.9431\u001b[0m |   \u001b[32m0.9254\u001b[0m |      \u001b[32m0.8743\u001b[0m | 2.370e-03 |  17.99 |   111100 | 1535.2655\n",
      "   12 |     \u001b[32m0.2643\u001b[0m |    \u001b[32m0.9686\u001b[0m |   \u001b[32m0.3629\u001b[0m |  \u001b[32m0.8988\u001b[0m |  \u001b[32m0.9436\u001b[0m |   \u001b[31m0.9217\u001b[0m |      \u001b[32m0.8782\u001b[0m | 1.778e-03 |  18.08 |   121200 | 1533.8976\n",
      "   13 |     \u001b[32m0.2472\u001b[0m |    \u001b[32m0.9705\u001b[0m |   \u001b[32m0.3509\u001b[0m |  \u001b[32m0.9027\u001b[0m |  \u001b[32m0.9459\u001b[0m |   \u001b[31m0.9221\u001b[0m |      \u001b[32m0.8852\u001b[0m | 1.185e-03 |  18.13 |   131300 | 1532.5618\n",
      "   14 |     \u001b[32m0.2339\u001b[0m |    \u001b[32m0.9720\u001b[0m |   \u001b[32m0.3435\u001b[0m |  \u001b[32m0.9046\u001b[0m |  \u001b[32m0.9470\u001b[0m |   \u001b[31m0.9250\u001b[0m |      \u001b[32m0.8862\u001b[0m | 5.926e-04 |  18.15 |   141400 | 1531.4463\n",
      "   15 |     \u001b[32m0.2231\u001b[0m |    \u001b[32m0.9734\u001b[0m |   \u001b[31m0.3435\u001b[0m |  \u001b[31m0.9025\u001b[0m |  \u001b[31m0.9460\u001b[0m |   \u001b[31m0.9239\u001b[0m |      \u001b[31m0.8832\u001b[0m | 0.000e+00 |  18.16 |   151500 | 1531.1572\n",
      "Loading /home/ytaille/christel_g/cache/train_step1/0493c15ed2180d03/checkpoint-15.pt... \n",
      "Model restored to its best self.state: 15\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"\", stream=sys.stdout)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "subs = [\n",
    "    (r\"(?<=[{}\\\\])(?![ ])\".format(string.punctuation), r\" \"),\n",
    "    (r\"(?<![ ])(?=[{}\\\\])\".format(string.punctuation), r\" \"),\n",
    "    (\"(?<=[a-zA-Z])(?=[0-9])\", r\" \"),\n",
    "    (\"(?<=[0-9])(?=[A-Za-z])\", r\" \"),\n",
    "    (\"[ ]{2,}\", \" \")\n",
    "]\n",
    "\n",
    "bert_name = \"bert-base-multilingual-uncased\"\n",
    "tg.set_device('cuda:0')\n",
    "device = tg.device\n",
    "\n",
    "# Step 1\n",
    "train_batcher, vocabularies, train_mentions, train_mention_ids, group_label_mask, quaero_batcher, quaero_mentions, quaero_mention_ids = preprocess(\n",
    "    bert_name=bert_name,\n",
    "    umls_versions=[\"2021AA\"],\n",
    "    source_full_lexicon=True,\n",
    "    source_lat=[\"FRE\"],\n",
    "    add_quaero_splits=[\"train\"],\n",
    "    other_full_lexicon=False,\n",
    "    other_lat=[\"ENG\"],\n",
    "    other_additional_labels=None,\n",
    "    other_mirror_existing_labels=True,\n",
    "    sty_groups=['ANAT', 'CHEM', 'DEVI', 'DISO', 'GEOG', 'LIVB', 'OBJC', 'PHEN', 'PHYS', 'PROC'],\n",
    "    other_sabs=None,\n",
    "    subs=subs,\n",
    "    apply_unidecode=True,\n",
    "    max_length=100,\n",
    ")\n",
    "\n",
    "history, classifier = train_step1(\n",
    "    # Data\n",
    "    train_batcher=train_batcher,\n",
    "    val_batcher=quaero_batcher[quaero_batcher['split'] == 0],\n",
    "    vocabularies=vocabularies,\n",
    "    group_label_mask=group_label_mask,\n",
    "\n",
    "    # Learning rates\n",
    "    metric_lr=8e-3,\n",
    "    inter_lr=8e-3,\n",
    "    bert_lr=2e-5,\n",
    "\n",
    "    # Misc params\n",
    "    metric='clustered_cosine',\n",
    "    dim=350,\n",
    "    rescale=20,\n",
    "    bert_name=bert_name,\n",
    "    batch_norm_affine=True,\n",
    "    batch_norm_momentum=0.1,\n",
    "    train_with_groups=True,\n",
    "\n",
    "    # Regularizers\n",
    "    dropout=0.2,\n",
    "    bert_dropout=0.2,\n",
    "    mask_and_shuffle=(2, 0.5, 0.1),\n",
    "    n_freeze=0,\n",
    "    sort_noise=1.,\n",
    "    n_neighbors=None,\n",
    "\n",
    "    # Scheduling\n",
    "    batch_size=128,\n",
    "    bert_warmup=0.1,\n",
    "    max_epoch=15,\n",
    "    decay_schedule=\"linear\",\n",
    "\n",
    "    # Experiment params\n",
    "    seed=123456,\n",
    "    stop_epoch=None,\n",
    "    with_cache=True,\n",
    "    debug=False,\n",
    "    from_tf=False,\n",
    "    with_tqdm=True,\n",
    ")\n",
    "\n",
    "\n",
    "# with open('/home/ytaille/AttentionSegmentation/vocab1.pkl', 'wb') as f:\n",
    "#     pkl.dump(vocabularies, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache /home/ytaille/christel_g/cache/preprocess_training_data/4d2b03f45f152b1f\n",
      "Loading /home/ytaille/christel_g/cache/preprocess_training_data/4d2b03f45f152b1f/output.pkl... \n",
      "Loading MRCONSO...\n",
      "Deduplicating MRCONSO...\n",
      "French synonyms: 389221\n",
      "French labels: 152156\n",
      "Quaero mentions: 5714\n",
      "Mirrored labels: 152828\n",
      "Queried english labels: 152828\n",
      "Adding all english concepts from SABs: ['CHV', 'SNOMEDCT_US', 'MTH', 'NCI', 'MSH']\n",
      "Total deduplicated synonyms: 3641321\n",
      "Total deduplicated labels: 1054783\n",
      "Will train vocabulary for label\n",
      "Will train vocabulary for group\n",
      "Will train vocabulary for source\n",
      "Will train vocabulary for token\n",
      "Discovered existing vocabulary (105879 entities) for token\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Quaero mentions: 16283\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized label, with given vocabulary and no unk\n",
      "Normalized quaero_source, with given vocabulary and no unk\n",
      "Normalized token, with given vocabulary and no unk\n",
      "Normalized group, with given vocabulary and no unk\n",
      "Normalized source, with given vocabulary and no unk\n",
      "Using cache /home/ytaille/christel_g/cache/train_step2/9e495908af591af7\n",
      "Loading /home/ytaille/christel_g/cache/train_step2/9e495908af591af7/output.pkl... \n",
      "epoch | val_acc | val_loss | acc_emea | acc_medline | train_loss | rescale |        lr\n",
      "    0 |  \u001b[32m0.6899\u001b[0m |     None |   \u001b[32m0.6725\u001b[0m |      \u001b[32m0.7033\u001b[0m |       None |    None |      None\n",
      "    1 |  \u001b[32m0.7855\u001b[0m |   \u001b[32m1.0014\u001b[0m |   \u001b[32m0.8138\u001b[0m |      \u001b[32m0.7638\u001b[0m |     \u001b[32m0.5895\u001b[0m | 20.0000 | 6.400e-03\n",
      "    2 |  \u001b[32m0.7960\u001b[0m |   \u001b[32m0.9880\u001b[0m |   \u001b[32m0.8304\u001b[0m |      \u001b[32m0.7695\u001b[0m |     \u001b[32m0.4146\u001b[0m | 20.0000 | 4.800e-03\n",
      "    3 |  \u001b[31m0.7941\u001b[0m |   \u001b[32m0.9877\u001b[0m |   \u001b[31m0.8286\u001b[0m |      \u001b[31m0.7675\u001b[0m |     \u001b[32m0.3657\u001b[0m | 20.0000 | 3.200e-03\n",
      "    4 |  \u001b[31m0.7956\u001b[0m |   \u001b[32m0.9842\u001b[0m |   \u001b[32m0.8321\u001b[0m |      \u001b[31m0.7675\u001b[0m |     \u001b[32m0.3398\u001b[0m | 20.0000 | 1.600e-03\n",
      "    5 |  \u001b[31m0.7956\u001b[0m |   \u001b[32m0.9836\u001b[0m |   \u001b[31m0.8304\u001b[0m |      \u001b[31m0.7688\u001b[0m |     \u001b[32m0.3238\u001b[0m | 20.0000 | 0.000e+00\n"
     ]
    }
   ],
   "source": [
    "# Step 2\n",
    "train_batcher, vocabularies, train_mentions, train_mention_ids, group_label_mask, quaero_batcher, quaero_mentions, quaero_mention_ids = preprocess(\n",
    "    bert_name=bert_name,\n",
    "    umls_versions=[\"2021AA\"],\n",
    "    source_full_lexicon=True,\n",
    "    source_lat=[\"FRE\"],\n",
    "    add_quaero_splits=[\"train\"],\n",
    "    other_full_lexicon=True,\n",
    "    other_lat=[\"ENG\"],\n",
    "    other_additional_labels=None,\n",
    "    other_mirror_existing_labels=True,\n",
    "    sty_groups=['ANAT', 'CHEM', 'DEVI', 'DISO', 'GEOG', 'LIVB', 'OBJC', 'PHEN', 'PHYS', 'PROC'],\n",
    "    other_sabs=[\"CHV\", \"SNOMEDCT_US\", \"MTH\", \"NCI\", \"MSH\"],\n",
    "    subs=subs,\n",
    "    apply_unidecode=True,\n",
    "    max_length=100,\n",
    ")\n",
    "\n",
    "# UNCOMMENT HERE TO SWITCH TO CPU\n",
    "\n",
    "# out_features est un paramètre pas mis à jour\n",
    "\n",
    "# classifier.cpu()\n",
    "classifier2 = train_step2(\n",
    "    classifier=torch_clone(classifier).to(tg.device),\n",
    "    train_batcher=train_batcher,\n",
    "    val_batcher=quaero_batcher[quaero_batcher['split'] == list(vocabularies['split']).index('dev')],\n",
    "    group_label_mask=group_label_mask,\n",
    "    batch_size=128,\n",
    "    sort_noise=1.,\n",
    "    decay_schedule=\"linear\",\n",
    "    lr=8e-3,\n",
    "    n_epochs=5,\n",
    "    seed=123456,\n",
    "    rescale=20,\n",
    "    n_neighbors=100,\n",
    ")\n",
    "\n",
    "# with open('/home/ytaille/AttentionSegmentation/vocab2.pkl', 'wb') as f:\n",
    "#     pkl.dump(vocabularies, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_name = \"bert-base-multilingual-uncased\"\n",
    "\n",
    "import os\n",
    "\n",
    "with open(os.path.join(VOCAB_PATH,'vocab1.pkl'), 'rb') as f:\n",
    "    vocabularies1 = pkl.load(f)\n",
    "\n",
    "with open(os.path.join(VOCAB_PATH,'vocab2.pkl'), 'rb') as f:\n",
    "    vocabularies2 = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will train vocabulary for label\n",
      "Normalized token, with given vocabulary and no unk\n",
      "Available CUDA devices: 1\n",
      "Current device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "dataset = load_from_brat(PREDICT_DATA_PATH)\n",
    "\n",
    "dataset['mentions']['mention_id'] = dataset['mentions']['doc_id'] +'.'+ dataset['mentions']['mention_id'].astype(str)\n",
    "dataset['fragments']['mention_id'] = dataset['fragments']['doc_id'] +'.'+ dataset['fragments']['mention_id'].astype(str)\n",
    "\n",
    "batcher, vocs, mention_ids = preprocess_train(\n",
    "    dataset,\n",
    "    vocabularies=vocabularies2,\n",
    "    bert_name=bert_name,\n",
    ")\n",
    "\n",
    "batch_size = len(batcher)\n",
    "with_tqdm = True\n",
    "\n",
    "tg.set_device('cuda:0') #('cuda:0')\n",
    "device = tg.device\n",
    "\n",
    "pred_batcher = predict(batcher, classifier2, batch_size=64, return_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.704225352112676,\n",
       " 'precision': 0.704225352112676,\n",
       " 'f1': 0.704225352112676,\n",
       " 'pred_count': 284.0,\n",
       " 'gold_count': 284.0,\n",
       " 'tp': 200.0,\n",
       " 'loss': 1.2823140587605222,\n",
       " 'map': 0.8039885340995149}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_batcher = predict(batcher, classifier2, batch_size=64, return_loss=True)\n",
    "compute_scores(pred_batcher, batcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANTRA\n",
    "\n",
    "# FR:\n",
    "# MEDLINE: 0.634\n",
    "# EMEA: 0.610\n",
    "\n",
    "# EN:\n",
    "# MEDLINE: 0.704\n",
    "# EMEA: 0.651\n",
    "\n",
    "# QUAERO:\n",
    "\n",
    "# MEDLINE: 0.708\n",
    "# EMEA: -> Duplicated mention_id, peut-être pas besoin de résoudre le pb..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_tokens = vocs['token']\n",
    "voc_labels = vocs['label']\n",
    "\n",
    "def replace_fn(s):\n",
    "    return s.replace(' ##', '').replace('[SEP]', '').replace('[PAD]', '').replace('[CLS]', '').strip()\n",
    "\n",
    "merged_batcher = batcher['mention',['mention_id','token']].merge(pred_batcher)\n",
    "\n",
    "final_tokens = [replace_fn(' '.join([voc_tokens[i] for i in b])) for b in merged_batcher['mention']['token'].toarray()]\n",
    "final_labels = [voc_labels[b] for b in merged_batcher['mention']['label']]\n",
    "\n",
    "final_couples = [(t, l) for t,l in zip(final_tokens, final_labels)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_brat(dataset, norm_labels, dest=None, filename_prefix=\"\"):\n",
    "    doc_id_to_text = dict(zip(dataset[\"docs\"][\"doc_id\"], dataset[\"docs\"][\"text\"]))\n",
    "    counter = 0\n",
    "    mention_counter = 0\n",
    "    mentions = dataset[\"mentions\"]\n",
    "    \n",
    "    if \"begin\" not in mentions:\n",
    "        mentions = mentions.merge(dataset[\"fragments\"])\n",
    "    if dest is not None:\n",
    "        try:\n",
    "            os.mkdir(dest)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "    \n",
    "    for doc_id, text in doc_id_to_text.items():\n",
    "        if not os.path.exists(\"{}/{}.txt\"):\n",
    "            with open(\"{}/{}.txt\".format(dest, filename_prefix + doc_id), \"w\") as f:\n",
    "                f.write(text)\n",
    "        doc_mentions = mentions[mentions.doc_id.str.contains(doc_id)]\n",
    "        counter += 1\n",
    "        f = None\n",
    "        if dest is not None:\n",
    "            f = open(\"{}/{}.ann\".format(dest, filename_prefix + doc_id), \"w\")\n",
    "        try:\n",
    "            mention_i = 0\n",
    "            for _, row in doc_mentions.iterrows():\n",
    "                mention_text = text[row[\"begin\"]:row[\"end\"]]\n",
    "                idx = row[\"begin\"]\n",
    "                mention_i += 1\n",
    "                spans = []\n",
    "                for part in mention_text.split(\"\\n\"):\n",
    "                    begin = idx\n",
    "                    end = idx + len(part)\n",
    "                    idx = end + 1\n",
    "                    if begin != end:\n",
    "                        spans.append((begin, end))\n",
    "                print(\"T{}\\t{} {}\\t{}\".format(\n",
    "                    mention_i,\n",
    "                    norm_labels[mention_counter][0], #str(row[\"label\"]),\n",
    "                    \";\".join(\" \".join(map(str, span)) for span in spans),\n",
    "                    mention_text.replace(\"\\n\", \" \")), file=f)\n",
    "                mention_counter += 1\n",
    "        finally:\n",
    "            if f is not None:\n",
    "                f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_to_brat(dataset, final_labels, dest=\"/home/ytaille/pyner/preds/medline_norm_preds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load quaero with CUI as labels instead of NER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_mult_norm",
   "language": "python",
   "name": "deep_mult_norm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
